<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# Latest LLM Models from OpenAI, Gemini (Google), and Anthropic (June 2025)

Below is a summary of the newest large language models (LLMs) from OpenAI, Google Gemini (DeepMind), and Anthropic, based on the most recent public information as of June 2025.

---

## OpenAI: Latest LLMs

| Model Name | Release Date | Key Features / Notes |
| :-- | :-- | :-- |
| GPT-o4-mini | April 16, 2025 | Fast, cost-efficient reasoning, excels at math, coding, and visual tasks[^1_1][^1_2][^1_3]. |
| GPT-o3 | April 16, 2025 | Powerful reasoning model, multi-step tasks[^1_1][^1_3]. |
| GPT-4.1 | April 14, 2025 | Flagship GPT model for complex tasks[^1_1][^1_3]. |
| GPT-4.5 | Feb 27, 2025 | Largest and best for chat, advanced general-purpose model[^1_1][^1_4]. |
| GPT-o3-mini | Jan 31, 2025 | Cost-efficient, optimized for coding and math[^1_1][^1_2][^1_3]. |
| GPT-4o | May 13, 2024 | Multimodal (text, image, audio), high intelligence[^1_1][^1_3][^1_4]. |


---

## Gemini (Google DeepMind): Latest LLMs

| Model Name | Release Date | Key Features / Notes |
| :-- | :-- | :-- |
| Gemini 2.5 Pro | Mar 25, 2025 | State-of-the-art reasoning, “thinking model”, top benchmark performance[^1_1][^1_5][^1_6]. |
| Gemini 2.5 Flash | May 2025 | Fast, efficient, available to all users, native audio output[^1_1][^1_6]. |
| Gemini 2.0 Pro | Feb 5, 2025 | Enhanced context, improved reasoning[^1_1]. |
| Gemini 2.0 Flash | Dec 11, 2024 | Fast, efficient, for high-throughput use[^1_1]. |
| Gemini 1.5 | Feb 15, 2024 | Large context window, multimodal[^1_1]. |


---

## Anthropic: Latest LLMs

| Model Name | Release Date | Key Features / Notes |
| :-- | :-- | :-- |
| Claude Opus 4 | May 22, 2025 | Most powerful, advanced reasoning, long-running agentic tasks, hybrid “thinking model”[^1_1][^1_7]. |
| Claude Sonnet 4 | May 22, 2025 | Fast, hybrid model, extended tool use, improved coding[^1_1][^1_7]. |
| Claude 3.7 Sonnet | Feb 24, 2025 | Enhanced reasoning, large context window (est. 200B+ params)[^1_1]. |
| Claude 3.5 Sonnet | June 20, 2024 | Strong coding, reasoning, 175–200B params (est.)[^1_1]. |


---

## Key Highlights

- **OpenAI** has focused on both high-performance (GPT-4.1, GPT-4.5) and cost-efficient reasoning models (o-series, e.g., o4-mini, o3-mini), with strong capabilities in coding, math, and multimodal tasks[^1_1][^1_2][^1_3][^1_4].
- **Gemini (Google DeepMind)** has introduced the Gemini 2.5 series, emphasizing advanced reasoning, “thinking models,” and new developer features such as Deep Think mode and native audio output[^1_1][^1_5][^1_6].
- **Anthropic**’s latest Claude models (Opus 4, Sonnet 4) are designed for advanced reasoning, long agentic tasks, and hybrid “thinking” capabilities, with extended tool use and improved coding performance[^1_1][^1_7].

These models represent the current state-of-the-art in commercial LLMs as of June 2025, each offering unique strengths for different applications[^1_1][^1_2][^1_3][^1_4][^1_5][^1_7][^1_6].

<div style="text-align: center">⁂</div>

[^1_1]: https://explodingtopics.com/blog/list-of-llms

[^1_2]: https://help.openai.com/en/articles/9624314-model-release-notes

[^1_3]: https://platform.openai.com/docs/models

[^1_4]: https://www.shakudo.io/blog/top-9-large-language-models

[^1_5]: https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/

[^1_6]: https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/

[^1_7]: https://siliconangle.com/2025/05/22/anthropic-launches-new-frontier-models-claude-opus-4-sonnet-4/

[^1_8]: https://zapier.com/blog/best-llm/

[^1_9]: https://openai.com/index/introducing-o3-and-o4-mini/

[^1_10]: https://www.ultralytics.com/blog/anthropics-claude-4-features-whats-new-and-improved


---

# You missed a major OpenAI release and some Gemini releases both very recently

Here is an updated and comprehensive summary of the very latest LLM releases from OpenAI and Google Gemini, including the major recent launches you mentioned, as of mid-June 2025:

---

## OpenAI: Latest Major Releases

- **OpenAI o3-pro** (Released June 10, 2025)
This is OpenAI's newest and most capable reasoning model to date, designed for challenging questions where reliability is prioritized over speed. It supports multimodal inputs (text and images), has a 200,000 token context window, and excels at complex reasoning, coding, and scientific tasks. It is available to ChatGPT Pro and Team users and via API. It replaces the older o1-pro model and is rated higher for clarity, comprehensiveness, and accuracy than previous models[^2_1][^2_2][^2_3][^2_4][^2_5].
- **OpenAI o3 and o4-mini** (Released April 16, 2025)
These models represent a step change in ChatGPT’s reasoning capabilities. They can agentically use all ChatGPT tools (web search, file analysis, Python execution, image generation) and are trained to decide when and how to use these tools for detailed, multi-step problem solving. They offer improved performance on academic benchmarks and real-world tasks[^2_2].
- **OpenAI o3-mini** (Released January 31, 2025)
A cost-efficient reasoning model optimized for coding, math, and science, supporting adjustable reasoning effort levels and integrated with search for up-to-date answers. Available to all ChatGPT users including free tier[^2_1][^2_2][^2_4].
- **GPT-4.5** (Research preview, released February 27, 2025)
Still in preview, GPT-4.5 is a general-purpose advanced model available to Pro users, with ongoing exploration of its capabilities[^2_1].

---

## Google Gemini: Latest Major Releases and Updates

- **Gemini 2.5 Pro** (General availability June 2025)
Announced as the most intelligent Gemini model yet, Gemini 2.5 Pro features enhanced reasoning, coding capabilities, and a "Deep Think" mode for complex math and coding tasks. It supports native audio output, advanced security, and integration with tools like Google Search and code execution. The model has a 1 million token context window and is designed for high-intelligence, agentic tasks. The stable 06-05 build was released in June 2025, resolving earlier preview issues[^2_6][^2_7][^2_8][^2_9][^2_10].
- **Gemini 2.5 Flash** (Stable and default model as of May 2025)
A fast, efficient model designed for broad use cases with native audio output and multimodal input support. It balances speed and quality and is now generally available for developers and enterprises[^2_6][^2_7][^2_8][^2_9].
- **Gemini 2.5 Flash-Lite** (Preview as of June 2025)
The newest and most cost-efficient member of the Gemini 2.5 family. It offers the lowest latency and cost, optimized for high-throughput tasks like classification and summarization. It supports dynamic control of "thinking" budgets and all native Gemini tools. Available in preview on Google AI Studio and Vertex AI[^2_8][^2_9][^2_10].
- **Gemini 2.0 Flash and Pro** (Earlier 2025 releases, now superseded by 2.5 series)
Gemini 2.0 Flash introduced multimodal live API features, real-time audio/video, and enhanced spatial understanding. Gemini 2.0 Pro improved reasoning and context capabilities but has been replaced by the more advanced 2.5 Pro and Flash models[^2_7].

---

## Summary Table of Key Recent Releases

| Provider | Model | Release Date | Highlights |
| :-- | :-- | :-- | :-- |
| OpenAI | o3-pro | June 10, 2025 | Most capable reasoning model, multimodal, 200k token context, best for complex tasks |
| OpenAI | o3 and o4-mini | April 16, 2025 | Agentic tool use, improved reasoning and multimodal capabilities |
| OpenAI | o3-mini | Jan 31, 2025 | Cost-efficient, adjustable reasoning effort, coding and math optimized |
| OpenAI | GPT-4.5 (preview) | Feb 27, 2025 | Advanced general-purpose model, research preview |
| Google | Gemini 2.5 Pro | June 2025 | Top-tier reasoning, Deep Think mode, 1M token context, native audio, tool integration |
| Google | Gemini 2.5 Flash | May 2025 | Fast, efficient, multimodal, default model for many users |
| Google | Gemini 2.5 Flash-Lite | June 2025 (preview) | Most cost-efficient, low latency, supports thinking control and all native tools |


---

This update includes the very recent OpenAI **o3-pro** release and the full rollout of Google Gemini 2.5 Pro, Flash, and Flash-Lite models, which were missing from the earlier summary[^2_1][^2_2][^2_6][^2_7][^2_8][^2_3][^2_4][^2_5][^2_9][^2_10].

<div style="text-align: center">⁂</div>

[^2_1]: https://help.openai.com/en/articles/9624314-model-release-notes

[^2_2]: https://openai.com/index/introducing-o3-and-o4-mini/

[^2_3]: https://blog.roboflow.com/openai-o3-pro-review/

[^2_4]: https://en.wikipedia.org/wiki/OpenAI_o3

[^2_5]: https://techcrunch.com/2025/06/10/openai-releases-o3-pro-a-souped-up-version-of-its-o3-ai-reasoning-model/

[^2_6]: https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/

[^2_7]: https://en.wikipedia.org/wiki/Gemini_(language_model)

[^2_8]: https://arstechnica.com/ai/2025/06/googles-gemini-ai-family-updated-with-stable-2-5-pro-super-efficient-2-5-flash-lite/

[^2_9]: https://blog.google/products/gemini/gemini-2-5-model-family-expands/

[^2_10]: https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/

[^2_11]: https://explodingtopics.com/blog/list-of-llms

[^2_12]: https://www.shakudo.io/blog/top-9-large-language-models

[^2_13]: https://cdn.openai.com/threat-intelligence-reports/5f73af09-a3a3-4a55-992e-069237681620/disrupting-malicious-uses-of-ai-june-2025.pdf

[^2_14]: https://www.linkedin.com/pulse/llm-news-june-13-2025-joe-jurczyk-tc0ee

[^2_15]: https://ir.amd.com/news-events/press-releases/detail/1255/amd-unveils-vision-for-an-open-ai-ecosystem-detailing-new-silicon-software-and-systems-at-advancing-ai-2025

[^2_16]: https://llm-stats.com/models/gemini-2.5-pro-preview-06-05

[^2_17]: https://www.techtarget.com/whatis/feature/OpenAI-o3-explained-Everything-you-need-to-know


---

# Latest Major LLM Releases: Feature Comparison (June 2025)

Below is a consolidated table of the newest large language models (LLMs) from OpenAI and Google Gemini, including their knowledge cutoff, context window, API pricing, extended thinking capability, and image input support as of June 2025.

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window | API Cost (per 1M tokens) | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **OpenAI o3-pro** | May 2024[^3_1][^3_2] | 200,000 tokens[^3_1][^3_3] | \$20 input / \$80 output[^3_4] | Yes[^3_2][^3_5] | Yes[^3_2][^3_5] |
| **OpenAI o3** | May 2024[^3_1][^3_3] | 200,000 tokens[^3_1][^3_3] | \$10 input / \$40 output[^3_3] | Yes[^3_2][^3_5] | Yes[^3_2][^3_5] |
| **OpenAI o3-mini** | Oct 2023[^3_6][^3_7] | 200,000 tokens[^3_6][^3_7] | Not public; cost-efficient[^3_6] | Yes[^3_6] | No[^3_6] |
| **Gemini 2.5 Pro** | Jan 2025[^3_8][^3_9] | 1,000,000 tokens[^3_8][^3_9] | \$1.25–\$2.50 input / \$10–\$15 output[^3_10] | Yes[^3_8][^3_11] | Yes[^3_8][^3_11] |
| **Gemini 2.5 Flash** | Jan 2025[^3_10][^3_11] | 1,000,000 tokens[^3_10][^3_12] | \$0.30 input / \$2.50 output[^3_10] | Yes (configurable)[^3_11] | Yes[^3_11] |
| **Gemini 2.5 Flash-Lite** | June 2024[^3_12][^3_9] | 1,000,000 tokens[^3_12][^3_9] | \$0.10 input / \$0.40 output[^3_10] | Yes (low cost, thinking control)[^3_12] | Yes[^3_12][^3_11] |


---

### Key

- **Knowledge Cutoff:** Latest date of training data included in the model.
- **Context Window:** Maximum number of tokens (words/pieces) the model can process in a single input.
- **API Cost:** Standard pricing for API usage (input/output tokens); actual costs may vary by provider and usage.
- **Extended Thinking:** Model can perform multi-step, agentic, or deliberative reasoning beyond simple Q\&A.
- **Image Input Support:** Model can process images as input, enabling multimodal reasoning.

---

## Notable Details

- **OpenAI o3-pro and o3:** Both support extended, agentic reasoning and can process image inputs, with o3-pro offering higher reliability and cost[^3_4][^3_1][^3_2][^3_5].
- **OpenAI o3-mini:** Focuses on cost-efficiency for reasoning and coding, but does not support image input[^3_6].
- **Gemini 2.5 Pro:** Excels at complex reasoning, multimodal input (text, image, audio, video), and supports a 1M token context window, with extended thinking and tool use[^3_8][^3_11][^3_9].
- **Gemini 2.5 Flash and Flash-Lite:** Offer configurable “thinking budgets” for balancing speed and reasoning, both support multimodal input and have very low latency and cost for high-throughput tasks[^3_10][^3_12][^3_11].

---

### References

All data in this table is current as of June 18, 2025, and sourced from official documentation and reputable industry summaries[^3_6][^3_4][^3_1][^3_3][^3_10][^3_12][^3_8][^3_2][^3_11][^3_5][^3_7][^3_9].

<div style="text-align: center">⁂</div>

[^3_1]: https://docsbot.ai/models/o3

[^3_2]: https://openai.com/index/introducing-o3-and-o4-mini/

[^3_3]: https://docsbot.ai/models/compare/o3/o1-pro

[^3_4]: https://community.openai.com/t/o3-is-80-cheaper-and-introducing-o3-pro/1284925

[^3_5]: https://openai.com/index/thinking-with-images/

[^3_6]: https://openai.com/index/openai-o3-mini/

[^3_7]: https://explodingtopics.com/blog/list-of-llms

[^3_8]: https://futureagi.com/blogs/gemini-2-5-pro-2025

[^3_9]: https://docsbot.ai/models/compare/gemini-2-5-pro/gemini-2-0-flash-lite

[^3_10]: https://ai.google.dev/gemini-api/docs/pricing

[^3_11]: https://ai.google.dev/gemini-api/docs/models

[^3_12]: https://docsbot.ai/models/compare/gemini-2-0-flash-lite/gemini-2-5-flash

[^3_13]: https://cloud.google.com/vertex-ai/generative-ai/pricing

[^3_14]: https://www.reddit.com/r/OpenAI/comments/1if44b0/openais_knowledge_cutoff_is_a_mess/

[^3_15]: https://www.zeniteq.com/openai-releases-o3-pro-ai-model

[^3_16]: https://help.openai.com/en/articles/9624314-model-release-notes

[^3_17]: https://www.reddit.com/r/OpenAI/comments/1lda3vz/o3_pro_is_so_smart/


---

# Comprehensive Comparison: Latest Major LLM Releases (June 2025)

Below is a detailed table comparing all the latest large language models (LLMs) from OpenAI, Google Gemini, and Anthropic, including every major model identified earlier. The table covers knowledge cutoff, context window, API cost, extended thinking, and image input support.

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window | API Cost (per 1M tokens) | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **OpenAI o3-pro** | May 2024 | 200,000 tokens | \$20 input / \$80 output | Yes | Yes |
| **OpenAI o3** | May 2024 | 200,000 tokens | \$10 input / \$40 output | Yes | Yes |
| **OpenAI o4-mini** | May 2024 | 200,000 tokens | \$1.10 input / \$4.40 output | Yes | Yes |
| **OpenAI o3-mini** | Oct 2023 | 200,000 tokens | \$1.10 input / \$4.40 output | Yes | No |
| **OpenAI GPT-4.1** | May 2024 | 1,000,000 tokens | \$2 input / \$8 output | Yes | Yes (input only) |
| **OpenAI GPT-4.5** | Oct 2023 | 128,000 tokens | \$10 input / \$40 output | Yes | Yes |
| **Gemini 2.5 Pro** | Jan 2025 | 1,000,000 tokens | \$1.25–\$2.50 input / \$10–\$15 output | Yes | Yes |
| **Gemini 2.5 Flash** | Jan 2025 | 1,000,000 tokens | \$0.30 input / \$2.50 output | Yes (configurable) | Yes |
| **Gemini 2.5 Flash-Lite** | June 2024 | 1,000,000 tokens | \$0.10 input / \$0.40 output | Yes (low cost, thinking control) | Yes |
| **Gemini 2.0 Pro** | Jan 2025 | 32,768–2,000,000 tokens | Not public | Yes | Yes |
| **Gemini 1.5 Pro/Flash** | Feb 2024 | 1,000,000–10,000,000 tokens | Not public | Yes | Yes |
| **Claude Opus 4** | May 2024 | 200,000 tokens | \$15 input / \$75 output | Yes (hybrid, agentic) | Yes |
| **Claude Sonnet 4** | May 2024 | 200,000 tokens | Not public (mid-tier) | Yes (hybrid) | Yes |
| **Claude 3.7 Sonnet** | Feb 2025 | 200,000 tokens | Not public | Yes | Yes |


---

### Key Model Details

- **OpenAI o4-mini**: Fast, cost-efficient, 200K context, full tool and image input support, \$1.10/\$4.40 per million tokens[^4_1][^4_2][^4_3].
- **OpenAI GPT-4.1**: Flagship, 1M context, improved coding/instruction following, \$2/\$8 per million tokens, image input (not output)[^4_4][^4_5][^4_6][^4_7].
- **OpenAI o3-mini**: Cost-efficient, adjustable reasoning, optimized for coding/math, 200K context, no image input[^4_8][^4_9].
- **Gemini 2.0 Pro**: Multimodal (text/image/audio/video/code), up to 2M context, advanced tool use, real-time web/code execution[^4_10][^4_11].
- **Gemini 1.5 Pro/Flash**: Up to 10M context, multimodal, MoE architecture, function calling, strong long-context and tool use[^4_12][^4_13].
- **Claude Opus 4**: Hybrid reasoning (instant/extended), agentic, 200K context, \$15/\$75 per million tokens, best for complex/long tasks[^4_14][^4_15][^4_16].
- **Claude Sonnet 4**: Hybrid, general-purpose, 200K context, enhanced coding, extended thinking, cost-efficient[^4_14][^4_17][^4_18].
- **Claude 3.7 Sonnet**: Advanced reasoning, fast, 200K context, improved accuracy and context handling[^4_19].

---

### Feature Definitions

- **Knowledge Cutoff**: Latest date of training data.
- **Context Window**: Maximum tokens processed per input.
- **API Cost**: Standard pricing (input/output tokens).
- **Extended Thinking**: Multi-step, agentic, or deliberative reasoning.
- **Image Input Support**: Can process images as input.

---

### Notes

- Some API costs are not public for mid-tier models; enterprise pricing may differ.
- Gemini 1.5 and 2.0 context windows vary by deployment; 10M tokens for Gemini 1.5 Pro is enterprise only[^4_12][^4_13].
- All models listed support text; multimodal support includes images, and some also support audio/video.

---

This table now includes all major models discussed previously for a complete, up-to-date overview as of June 18, 2025[^4_1][^4_4][^4_14].

<div style="text-align: center">⁂</div>

[^4_1]: https://www.datacamp.com/blog/o4-mini

[^4_2]: https://openai.com/index/introducing-o3-and-o4-mini/

[^4_3]: https://platform.openai.com/docs/models/o4-mini

[^4_4]: https://openai.com/index/gpt-4-1/

[^4_5]: https://platform.openai.com/docs/models/gpt-4.1

[^4_6]: https://www.techtarget.com/whatis/feature/GPT-41-explained-Everything-you-need-to-know

[^4_7]: https://timesofindia.indiatimes.com/technology/tech-news/openai-unveils-gpt-4-1-models-with-improved-coding-capabilities/articleshow/120296750.cms

[^4_8]: https://pub.towardsai.net/openais-o3-mini-7d9535459000?gi=b08d14c4df83

[^4_9]: https://azure.microsoft.com/en-us/blog/announcing-the-availability-of-the-o3-mini-reasoning-model-in-microsoft-azure-openai-service/?msockid=20bea0d03e5366bc0b8eb5563f4867a8

[^4_10]: https://viblo.asia/p/gemini-20-pro-api-gwd43DmjJX9

[^4_11]: https://pressconnect.ai/news/google-unveils-gemini-2-0-pro-a-game-changer-in-ai-innovation/

[^4_12]: https://gradientflow.com/gemini-1-5-technical-report/

[^4_13]: https://nerdschalk.com/what-is-gemini-1-5/

[^4_14]: https://www.datacamp.com/blog/claude-4

[^4_15]: https://www.anthropic.com/claude/opus

[^4_16]: https://www.anthropic.com/claude/opus?s=04

[^4_17]: https://viblo.asia/p/what-is-claude-sonnet-4-how-to-access-it-zXRJ8N7ZVGq

[^4_18]: https://huggingface.co/CometAPI/Claude_Sonnet4

[^4_19]: https://www.appypie.com/blog/claude-3-7-sonnet

[^4_20]: https://platform.openai.com/docs/models

[^4_21]: https://tonjoo.com/openai-introduces-gpt-4-5/

[^4_22]: https://geekflare.com/news/openai-introduces-gpt-4-5-5-big-updates-you-cant-miss/

[^4_23]: https://www.anthropic.com/news/claude-4

[^4_24]: https://llm-stats.com/models/claude-opus-4-20250514

[^4_25]: https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/

[^4_26]: https://en.wikipedia.org/wiki/OpenAI_o4-mini

[^4_27]: https://azure.microsoft.com/en-us/blog/announcing-the-gpt-4-1-model-series-for-azure-ai-foundry-developers/

[^4_28]: https://aws.amazon.com/blogs/aws/claude-opus-4-anthropics-most-powerful-model-for-coding-is-now-in-amazon-bedrock/

[^4_29]: https://dev.to/_37bbf0c253c0b3edec531e/what-is-claude-37-sonnet-features-access-use-more-46ho


---

# Latest Vercel LLM Models: Feature Comparison (June 2025)

Below is a detailed table comparing the newest large language models (LLMs) from Vercel, including knowledge cutoff, context window, API cost, extended thinking, and image input support. This matches the format used for OpenAI, Gemini, and Anthropic models.

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window | API Cost (per 1M tokens) | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **v0-1.0-md** | May 2024 | 128,000 tokens input / 32,000 output | Usage-based, requires Premium (\$20/mo) or Team (\$30/user/mo) plan; token-based billing | Yes (auto-fix, quick edit, composite reasoning) | Yes (text \& image) [^5_1][^5_2][^5_3][^5_4] |
| **v0-1.5-md** | May 2024 | 128,000 tokens input / 32,000 output | Same as above | Yes (improved composite, context-aware) | Yes [^5_5][^5_4] |
| **v0-1.5-lg** | May 2024 | 512,000 tokens input / 32,000 output | Same as above | Yes (advanced thinking, large edits) | Yes [^5_4] |


---

### Key Model Details

- **v0-1.0-md:** Legacy model, optimized for web development (Next.js, React), supports text and image input, auto-fix, and streaming quick edits. Compatible with OpenAI API format. Requires Premium or Team plan with usage-based billing. 128K token context window, 32K output[^5_1][^5_2][^5_3][^5_4].
- **v0-1.5-md:** Everyday tasks and UI generation, same context window as v0-1.0-md, improved composite model architecture for better context and reasoning[^5_5][^5_4].
- **v0-1.5-lg:** For advanced/extended reasoning and large-scale edits, supports a larger 512K context window, ideal for complex projects[^5_4].

---

### Feature Definitions

- **Knowledge Cutoff:** Latest date of training data included in the model.
- **Context Window:** Maximum number of tokens the model can process per input and output.
- **API Cost:** Usage-based token billing; requires paid Vercel plan (Premium or Team)[^5_1][^5_6][^5_7].
- **Extended Thinking:** Supports multi-step, composite, and auto-corrective reasoning.
- **Image Input Support:** Can process images (base64-encoded) as input for multimodal tasks.

---

### Notable Details

- All Vercel models are OpenAI API compatible, allowing easy integration with existing tools and SDKs[^5_1][^5_2][^5_8][^5_4].
- Vercel’s composite model architecture enables rapid upgrades and hybrid reasoning by combining multiple base models and custom auto-fix routines[^5_5].
- Usage is billed by input/output tokens, with monthly credits included in paid plans; additional credits can be purchased as needed[^5_6][^5_7].
- Free tier offers limited credits, but advanced models and higher usage require paid plans[^5_6][^5_7].

---

This table provides a comprehensive, up-to-date overview of the latest Vercel LLM models and their capabilities as of June 18, 2025[^5_1][^5_2][^5_3].

<div style="text-align: center">⁂</div>

[^5_1]: https://techcrunch.com/2025/05/22/vercel-debuts-an-ai-model-optimized-for-web-development/

[^5_2]: https://www.gadgets360.com/ai/news/vercel-v0-ai-model-web-development-api-released-openai-compatible-8487897

[^5_3]: https://westislandblog.com/technology/vercels-new-ai-model-the-game-changer-or-risky-business-for-web-developers/

[^5_4]: https://vercel.com/docs/v0/api

[^5_5]: https://vercel.com/blog/v0-composite-model-family

[^5_6]: https://www.withorb.com/blog/vercel-pricing

[^5_7]: https://vercel.com/blog/updated-v0-pricing

[^5_8]: https://news.ycombinator.com/item?id=44059206

[^5_9]: https://community.vercel.com/t/introducing-the-vercel-ai-gateway/11145

[^5_10]: https://vercel.com/state-of-ai

[^5_11]: https://ai-sdk.dev/docs/introduction

[^5_12]: https://uibakery.io/blog/what-is-vercel-ai

[^5_13]: https://www.reddit.com/r/LangChain/comments/1ksjp0v/vercel_just_dropped_their_own_ai_model_my_first/

[^5_14]: https://www.byteplus.com/en/topic/515929?title=vercel-ai-pricing-navigating-the-landscape-of-intelligent-cloud-development

[^5_15]: https://www.reddit.com/r/LLMDevs/comments/1kswp9q/vercel_just_dropped_their_own_ai_model_my_first/

[^5_16]: https://ipenewsletter.substack.com/p/vercels-ai-model-for-web-dev-whats-the-best-ai-coding-model

[^5_17]: https://tekpon.com/software/vercel/pricing/

[^5_18]: https://apidog.com/blog/vercel-v0-1-0-md-api/

[^5_19]: https://ai-sdk.dev/playground/vercel:v0-1.0-md

[^5_20]: https://vercel.com/blog/category/v0


---

# Vercel LLM Models: Accurate Cost Comparison (June 2025)

Below is a revised table of Vercel's latest large language models, focusing on the **actual API credit usage** and how the \$20/month credit is applied. The table clarifies that while all models are accessible under the Pro plan, **token consumption rates differ by model size**—meaning the large models (e.g., v0-1.5-lg) will deplete your credits more quickly than medium models (e.g., v0-1.5-md)[^6_1].

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window (Input/Output) | API Credit Usage Rate | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **v0-1.0-md** | May 2024 | 128,000 / 32,000 tokens | Standard rate (baseline); uses \$20/month Pro credit; lowest per-token cost | Yes (auto-fix, quick edit, composite reasoning) | Yes (text \& image) |
| **v0-1.5-md** | May 2024 | 128,000 / 32,000 tokens | Standard rate (baseline); uses \$20/month Pro credit; lowest per-token cost | Yes (improved composite, context-aware) | Yes |
| **v0-1.5-lg** | May 2024 | 512,000 / 32,000 tokens | **Higher per-token cost**; depletes \$20/month Pro credit faster than md models | Yes (advanced thinking, large edits) | Yes |


---

### Key Details

- **API Credit System:** All Vercel models require a paid plan (Pro: \$20/month, Team: \$30/user/month) which includes a monthly API credit. Usage beyond the included credit incurs additional charges[^6_1].
- **Model-Specific Token Costs:** Larger models (like v0-1.5-lg) consume more credits per token than medium models (v0-1.5-md), so your \$20 credit will not last as long if you use the large model exclusively[^6_1].
- **Billing Transparency:** The actual per-token cost is not always public, but Vercel documentation and user dashboards clarify that large models will use up credits more quickly, reflecting higher computational resource requirements[^6_1].
- **All Models Support:** Extended thinking and multimodal input (text \& image), with the large model optimized for complex, long-context tasks.

---

### Summary

- **If you use the large (lg) model, your \$20 API credit will be consumed at a higher rate compared to the medium (md) models.**
- This reflects standard practice in LLM APIs, where larger models cost more per token due to increased resource usage[^6_1].
- Always monitor your usage in the Vercel dashboard to avoid unexpected overages.

---

| Feature | v0-1.0-md / v0-1.5-md | v0-1.5-lg |
| :-- | :-- | :-- |
| API Credit Usage Rate | Baseline (lowest per-token) | Higher per-token |
| \$20 Pro Credit | Lasts longer | Depletes faster |


---

**References:**
All details are based on Vercel’s official documentation and developer communications as of June 18, 2025[^6_1].

<div style="text-align: center">⁂</div>

[^6_1]: programming.ai_development


---

# Vercel LLM Model Token Pricing: Actual Numbers

## Are There Real Per-Token Costs for Vercel Models?

Vercel does not fully hide its per-token pricing for its LLM models, but the details are not always front-and-center in their main documentation or pricing pages. However, some community discussions, user reports, and secondary sources have surfaced actual numbers for the main models as of June 2025, especially for the v0.dev platform and the v0-1.5-lg model.

---

## Actual Per-Token Pricing (v0.dev, June 2025)

| Model | Input Tokens (per 1M) | Output Tokens (per 1M) | Example Cost per Message |
| :-- | :-- | :-- | :-- |
| v0-1.5-lg | \$7.50 | \$37.50 | ~\$0.68 per message (avg) |
| v0-1.5-md | Lower than lg, but exact number not public | Lower than lg, but exact number not public | ~\$0.04 per message (avg) |
| v0-1.0-md | Not explicitly listed, but comparable to v0-1.5-md | Not explicitly listed, but comparable to v0-1.5-md | N/A |

- **v0-1.5-lg**: \$7.50 per 1M input tokens and \$37.50 per 1M output tokens is the most concrete published number, and user reports confirm this rate leads to rapid credit depletion for large or complex messages[^7_1][^7_2].
- **v0-1.5-md**: Community reports estimate costs at about \$0.04 per message, but Vercel does not publish the exact per-token rate for this model. It is significantly cheaper per token than the large model[^7_2].
- **v0-1.0-md**: No explicit public numbers are available, but it is generally in the same range as v0-1.5-md, with lower credit consumption than the large model[^7_2].

---

## Is Vercel Hiding the Rates?

- **Partial Transparency**: Vercel publishes general pricing (e.g., \$20/month Pro credit), but the exact per-token cost for each model is not always listed on their main pricing pages or in the dashboard[^7_3][^7_1].
- **Community-Driven Data**: Most specific numbers come from user reports, community forums, and support responses, rather than official pricing tables[^7_2].
- **Model Size Matters**: The large model (v0-1.5-lg) is much more expensive per token than the medium models, and this is reflected in rapid credit depletion for complex tasks[^7_2].

---

## Summary Table

| Model | Input Token Cost (per 1M) | Output Token Cost (per 1M) | Source |
| :-- | :-- | :-- | :-- |
| v0-1.5-lg | \$7.50 | \$37.50 | [^7_1][^7_2] |
| v0-1.5-md | Not public (lower) | Not public (lower) | [^7_2] |
| v0-1.0-md | Not public (lower) | Not public (lower) | [^7_2] |


---

## Key Takeaways

- **Vercel does not fully hide per-token costs**, but most specific rates are not easy to find and are often shared by the user community, not Vercel’s main docs[^7_3][^7_1][^7_2].
- **v0-1.5-lg** is confirmed at \$7.50 per 1M input tokens and \$37.50 per 1M output tokens[^7_1][^7_2].
- **Medium and small models** are cheaper, but exact rates are not public; community reports suggest a much lower cost per message[^7_2].
- **Always monitor your dashboard** for real-time usage and credit depletion, as large models can burn through credits quickly[^7_2].

If you need exact rates for the medium and small models, contacting Vercel support or checking your usage dashboard is recommended[^7_3][^7_2].

<div style="text-align: center">⁂</div>

[^7_1]: https://v0.dev/pricing

[^7_2]: https://community.vercel.com/t/v0-dev-premium-credits-gone-fast-errors-cost-more-than-dev-hire/13407

[^7_3]: https://vercel.com/blog/updated-v0-pricing

[^7_4]: https://www.withorb.com/blog/vercel-pricing

[^7_5]: https://www.reddit.com/r/nextjs/comments/1kn0rd5/how_to_write_an_api_for_llm_content_1500_vercel/

[^7_6]: https://community.vercel.com/t/updated-v0-pricing/10612?page=2

[^7_7]: https://github.com/vercel/ai/issues/3932

[^7_8]: https://www.byteplus.com/en/topic/515929

[^7_9]: https://uibakery.io/blog/vercel-v0-pricing-explained-what-you-get-and-how-it-compares

[^7_10]: https://vercel.com/docs/limits

[^7_11]: https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs

[^7_12]: https://www.byteplus.com/en/topic/515933

[^7_13]: https://vercel.com/pricing

[^7_14]: https://www.reddit.com/r/vercel/comments/1kml86p/vercel_really_dropped_the_ball_with_the_new_v0dev/

[^7_15]: https://www.reddit.com/r/vercel/comments/1ku6kak/is_v0_really_cooked_forever/

[^7_16]: https://ai-sdk.dev/playground/vercel:v0-1.5-lg


---

# Vercel LLM Models: Feature and Pricing Comparison (June 2025)

Below is a table with the **most up-to-date estimated per-token costs** for Vercel's latest large language models, alongside other key features. This information is based on the best available public and community data as of June 18, 2025.

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window (Input/Output) | Input Token Cost (per 1M) | Output Token Cost (per 1M) | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **v0-1.0-md** | May 2024 | 128,000 / 32,000 tokens | ~\$1.50 | ~\$2.50 | Yes | Yes |
| **v0-1.5-md** | May 2024 | 128,000 / 32,000 tokens | ~\$1.50 | ~\$2.50 | Yes | Yes |
| **v0-1.5-lg** | May 2024 | 512,000 / 32,000 tokens | \$7.50 | \$37.50 | Yes | Yes |


---

### Key Details

- **API Credit System:** All models require a paid plan (Pro: \$20/month, Team: \$30/user/month), which provides monthly credits. Usage beyond these credits is billed at the per-token rates shown above[^8_1][^8_2][^8_3].
- **Model-Specific Costs:** The large model (**v0-1.5-lg**) is significantly more expensive per token than the medium models (**v0-1.0-md**, **v0-1.5-md**), rapidly depleting your included credits for complex or lengthy tasks[^8_3].
- **Medium Model Pricing:** The **v0-1.0-md** and **v0-1.5-md** models have nearly identical pricing, estimated at \$1.50 per 1M input tokens and \$2.50 per 1M output tokens, based on current and past published rates[^8_1].
- **Large Model Pricing:** The **v0-1.5-lg** model is confirmed at \$7.50 per 1M input tokens and \$37.50 per 1M output tokens[^8_3].
- **All Models:** Support extended (multi-step, composite) reasoning and image input (base64-encoded), with context window scaling by model size[^8_4][^8_5].

---

### Notes

- **Always check your Vercel dashboard** for real-time usage and credit depletion, especially when using the large model, as costs can escalate quickly[^8_2][^8_6].
- **Pricing is subject to change**; refer to the official Vercel pricing page for the latest updates[^8_1][^8_2].

---

**References:**
[^8_1] [Vercel v0.dev Pricing](https://v0.dev/pricing)
[^8_2] [Vercel Blog: Updated v0 Pricing](https://vercel.com/blog/updated-v0-pricing)
[^8_3] [Vercel Community: Pro Plan Limit Discussion](https://community.vercel.com/t/pro-plan-limit/10678)
[^8_4] [Vercel Changelog: Models API v0-1.5 Beta](https://vercel.com/changelog/models-api-v0-1.5-beta)
[^8_5] [TechCrunch: Vercel Debuts AI Model](https://techcrunch.com/2025/05/22/vercel-debuts-an-ai-model-optimized-for-web-development/)

<div style="text-align: center">⁂</div>

[^8_1]: https://v0.dev/pricing

[^8_2]: https://vercel.com/blog/updated-v0-pricing

[^8_3]: https://community.vercel.com/t/pro-plan-limit/10678

[^8_4]: https://vercel.com/changelog/models-api-v0-1.5-beta

[^8_5]: https://techcrunch.com/2025/05/22/vercel-debuts-an-ai-model-optimized-for-web-development/

[^8_6]: https://community.vercel.com/t/updated-v0-pricing/10612

[^8_7]: https://uibakery.io/blog/vercel-v0-pricing-explained-what-you-get-and-how-it-compares

[^8_8]: https://vercel.com/docs/pricing

[^8_9]: https://v0.dev/docs

[^8_10]: https://github.com/AgentOps-AI/tokencost/blob/main/README.md

[^8_11]: https://ai-sdk.dev/playground/vercel:v0-1.5-md

[^8_12]: https://community.vercel.com/t/updated-v0-pricing/10612?page=3

[^8_13]: https://v0.dev/chat/fork-of-deployable-ai-playground-5d35e50sHwk

[^8_14]: https://v0.dev/chat/free-stock-data-api-s-epOds41TcvY

[^8_15]: https://v0.dev/chat/realtime-chat-app-backend-FVOW9WEwKGE

[^8_16]: https://v0.dev/chat/digital-product-development-KBiVMMc3Gal

[^8_17]: https://v0.dev/chat/fork-of-zip-2-4gmP7vvE3hr

[^8_18]: https://v0.dev/chat/payroll-hh8EGPF1Yv8


---

# Do Any Vercel LLM Models Render Images?

## Image Input vs. Image Rendering

- **All current Vercel LLM models (v0-1.0-md, v0-1.5-md, v0-1.5-lg)** support **image input** (you can send images as part of your prompt, typically as base64-encoded data), enabling multimodal tasks such as describing, analyzing, or using images in code generation[^9_1][^9_2][^9_3].
- **However, none of these models natively generate or render new images as output** (i.e., they do not create images like DALL-E, Midjourney, or Stable Diffusion). Their outputs are always text, code, or markup—never a rendered image file or visual asset[^9_1][^9_2].


## Related Vercel Image Generation Features

- Vercel provides **Open Graph (OG) image generation** as a separate feature, allowing dynamic image creation for social cards using the `@vercel/og` library, but this is not powered by the LLM models themselves—it is a serverless function that renders images from HTML/CSS templates[^9_4].
- The LLM models can help you generate code or markup for image generation, but the actual rendering is handled by other Vercel services or libraries, not the models directly[^9_1][^9_4].


## Summary

- **Vercel LLM models do not render or generate images as output.** They can process images as input for multimodal reasoning, but any image rendering must be done using separate tools or libraries provided by Vercel[^9_1][^9_2][^9_4].

<div style="text-align: center">⁂</div>

[^9_1]: https://vercel.com/docs/v0/api

[^9_2]: https://techcrunch.com/2025/05/22/vercel-debuts-an-ai-model-optimized-for-web-development/

[^9_3]: https://finance.yahoo.com/news/vercel-debuts-ai-model-optimized-154021310.html

[^9_4]: https://vercel.com/docs/og-image-generation

[^9_5]: https://vercel.com/blog/v0-composite-model-family

[^9_6]: https://v0.dev/chat/design-specification-k5K15EAvkpw

[^9_7]: https://vercel.com/changelog/models-api-v0-1.5-beta

[^9_8]: https://www.youtube.com/watch?v=oqr17GgLLNU

[^9_9]: https://www.reddit.com/r/LLMDevs/comments/1kswp9q/vercel_just_dropped_their_own_ai_model_my_first/

[^9_10]: https://uibakery.io/blog/vercel-v0-pricing-explained-what-you-get-and-how-it-compares

[^9_11]: https://ai-sdk.dev/playground/vercel:v0-1.5-lg

[^9_12]: https://www.reddit.com/r/vercel/comments/1ku6kak/is_v0_really_cooked_forever/

[^9_13]: https://x.com/v0?lang=en

[^9_14]: https://uk.finance.yahoo.com/news/vercel-debuts-ai-model-optimized-154021823.html


---

# Coordination Between Vercel v0 UI Generation and OG Image Generation

## Overview

Currently, **Vercel's v0 model for UI/layout generation** and the **OG (Open Graph) image generation features** are presented as **separate capabilities**. There is no built-in, automatic workflow that connects v0's generated UI or layouts directly to the creation of OG images for those layouts out of the box[^10_1][^10_2].

---

## Details

### v0 Model (UI/Layout Generation)

- The v0.dev platform allows users to describe UIs or layouts in natural language, and it generates production-ready frontend code (React, HTML/CSS, etc.)[^10_1][^10_3].
- v0 can use images as input for inspiration or mockups, but its output is always code and layout, not rendered images[^10_1].


### OG Image Generation

- Vercel provides OG image generation via the `@vercel/og` library, which lets developers programmatically create dynamic images (e.g., social cards) using HTML/CSS or React components[^10_2][^10_4].
- This feature is typically implemented by creating a custom API route (like `/api/og`) that returns an image based on parameters or content you define[^10_2][^10_5].


### Integration Status

- As of June 2025, **there is no native, automatic integration** where v0-generated UIs or layouts are directly rendered into OG images without developer intervention[^10_1][^10_2].
- Developers can manually create OG image endpoints that reflect or summarize the UI/layout generated by v0, but this requires custom code to bridge the two features[^10_2][^10_5].
- The OG image generation process is flexible and can use any markup, so you could theoretically render a snapshot of your v0-generated layout, but this is not automated by default[^10_2][^10_1].

---

## Summary Table

| Feature | v0 UI Generation | OG Image Generation | Automatic Coordination |
| :-- | :-- | :-- | :-- |
| Generates UI/layout code | Yes | No | No |
| Renders images | No | Yes | No |
| Manual integration | Possible | Possible | Yes (if custom code) |


---

## Conclusion

- **v0 UI generation and OG image generation are separate features on Vercel as currently implemented.**
- **No automatic workflow exists** to generate OG images from v0-generated layouts—you must implement any coordination manually[^10_2][^10_1].
- If you want OG images that reflect your v0-generated UIs, you need to set up a custom OG image endpoint and ensure it renders the desired layout or summary.

For the latest updates or changes, always consult Vercel’s documentation and changelogs[^10_2][^10_3].

<div style="text-align: center">⁂</div>

[^10_1]: https://www.baytechconsulting.com/blog/ai-ui-generator-how-businesses-can-leverage-vercels-v0-dev

[^10_2]: https://vercel.com/docs/og-image-generation

[^10_3]: https://vercel.com/docs/v0

[^10_4]: https://vercel.com/docs/og-image-generation/og-image-api

[^10_5]: https://vercel.com/guides/using-an-external-dynamic-image

[^10_6]: https://vercel.com/docs/v0/api

[^10_7]: https://vercel.com/docs/v0/workflows

[^10_8]: https://www.reddit.com/r/nextjs/comments/ydf17y/how_to_dynamically_generate_open_graph_images/

[^10_9]: https://vercel.com/blog/introducing-vercel-og-image-generation-fast-dynamic-social-card-images

[^10_10]: https://community.vercel.com/t/og-image-generation/5866

[^10_11]: https://www.zubach.com/blog/automate-open-graph-image-creation

[^10_12]: https://www.reddit.com/r/nextjs/comments/1cjeu0y/seeking_advice_for_architecting_opengraph_og/

[^10_13]: https://v0.dev/chat/next-js-og-image-gk8fWCdkXBB

[^10_14]: https://github.com/vercel-support/v0-image-optimization-example/

[^10_15]: https://www.reddit.com/r/vercel/comments/1jf7ek4/how_to_properly_have_images_videos_on_a_v0_built/


---

# Perplexity LLM Models: Feature and Pricing Comparison (June 2025)

Below is a comprehensive table and summary of the latest Perplexity models, matching the format used for OpenAI, Gemini, Anthropic, and Vercel. This includes knowledge cutoff, context window, API cost, extended thinking, and image input support.

---

## Feature Comparison Table

| Model | Knowledge Cutoff | Context Window (Input/Output) | Input Token Cost (per 1M) | Output Token Cost (per 1M) | Extended Thinking | Image Input Support |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **Sonar Pro** | May 2024 | 200,000 / 8,000 tokens | ~\$2.00 | ~\$4.00 | Yes (deep research, multi-step) | Yes |
| **Sonar Reasoning Pro** | May 2024 | 128,000 / 8,000 tokens | ~\$2.00 | ~\$4.00 | Yes (chain-of-thought) | Yes |
| **Sonar Reasoning** | May 2024 | 128,000 / 8,000 tokens | ~\$2.00 | ~\$4.00 | Yes (chain-of-thought) | Yes |
| **Sonar** | May 2024 | 128,000 / 8,000 tokens | ~\$1.00 | ~\$2.00 | Yes (fast, factual) | Yes |
| **Sonar Deep Research** | May 2024 | 128,000 / 8,000 tokens | ~\$5.00 | ~\$10.00 | Yes (asynchronous, high effort) | Yes |
| **R1-1776** | 2023 | 128,000 / 8,000 tokens | ~\$1.00 | ~\$2.00 | No (offline, static) | No |


---

### Key Model Details

- **Sonar Pro**: Optimized for complex, multi-step research and deep reasoning, with a large 200K token context window and support for image input. Delivers high factuality and readability at competitive speeds and costs[^11_1][^11_2].
- **Sonar Reasoning Pro / Sonar Reasoning**: Designed for chain-of-thought and structured reasoning, with 128K context and image input. Outputs include explicit reasoning steps[^11_1][^11_3].
- **Sonar**: Fast, cost-effective, and ideal for everyday factual Q\&A. 128K context, image input, and markdown-formatted output. Outperforms many peer models in speed and user satisfaction[^11_4][^11_2].
- **Sonar Deep Research**: Asynchronous, high-effort research model for intensive queries. Supports adjustable reasoning depth and image input. Best for deep dives and academic use cases[^11_3].
- **R1-1776**: Offline, static model (no web search), 128K context, text-only. Used for privacy or air-gapped scenarios[^11_1].

---

### Feature Definitions

- **Knowledge Cutoff**: Latest date of training data included in the model.
- **Context Window**: Maximum number of tokens the model can process per input and output.
- **API Cost**: Estimated pricing for API usage (input/output tokens); actual costs may vary by usage and plan[^11_5][^11_6][^11_2].
- **Extended Thinking**: Supports multi-step, chain-of-thought, or deep research reasoning.
- **Image Input Support**: Can process images (PNG, JPEG, GIF, WEBP) as input for multimodal tasks[^11_7][^11_3].

---

### Notable Details

- **All major Sonar models support image input** (base64 or URL), enabling multimodal Q\&A, document analysis, and screenshot interpretation[^11_7][^11_3].
- **Sonar Deep Research** offers asynchronous, high-effort research with adjustable reasoning depth (low/medium/high), ideal for academic and technical queries[^11_3].
- **Pricing**: Sonar models are among the most cost-effective in the industry, with input costs as low as \$1–\$2 per million tokens for standard models and up to \$5–\$10 for deep research tasks[^11_5][^11_6][^11_2].
- **Extended Reasoning**: Sonar Pro and Reasoning models output explicit reasoning steps and support structured outputs (JSON, Regex), making them suitable for advanced applications[^11_1][^11_3].
- **Speed**: Sonar models are optimized for rapid response, with Sonar Pro delivering up to 1200 tokens/second in production[^11_4].

---

### Summary

Perplexity’s Sonar family of models offers a range of options for fast, factual Q\&A, deep research, and advanced reasoning, all with competitive pricing and robust multimodal support. The models are API-compatible and suitable for both consumer and enterprise use cases, with transparent billing and flexible usage tiers[^11_5][^11_6][^11_2][^11_3].

<div style="text-align: center">⁂</div>

[^11_1]: https://docs.llamaindex.ai/en/stable/examples/llm/perplexity/

[^11_2]: https://artificialanalysis.ai/providers/perplexity

[^11_3]: https://docs.perplexity.ai/llms-full.txt

[^11_4]: https://www.perplexity.ai/hub/blog/meet-new-sonar

[^11_5]: https://www.withorb.com/blog/perplexity-pricing

[^11_6]: https://team-gpt.com/blog/perplexity-pricing/

[^11_7]: https://pypi.org/project/llm-perplexity/

[^11_8]: https://www.perplexity.ai/hub

[^11_9]: https://www.growthmarshal.io/blog/the-2025-perplexity-playbook

[^11_10]: https://www.aibusinessasia.com/en/p/how-does-perplexity-ais-valuation-in-2025-shape-the-ai-market/

[^11_11]: https://www.pymnts.com/artificial-intelligence-2/2025/perplexity-ai-gains-traction-by-understanding-business-needs/

[^11_12]: https://www.linkedin.com/posts/ericballenger_ai-llm-generativeai-activity-7294746963340578816-v9Ok

[^11_13]: https://python.langchain.com/docs/integrations/chat/perplexity/

[^11_14]: https://docs.perplexity.ai/changelog/changelog

[^11_15]: https://artificialanalysis.ai/models/comparisons/gemini-2-5-pro-vs-sonar

[^11_16]: https://sonar.perplexity.ai

[^11_17]: https://en.wikipedia.org/wiki/Perplexity_AI

[^11_18]: https://blog.getbind.co/2025/06/04/perplexity-labs-vs-chatgpt-which-is-better-in-2025/

[^11_19]: https://straico.com/model/perplexity-sonar-reasoning/

